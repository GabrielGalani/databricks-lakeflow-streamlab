# üöÄ Data Pipeline: E-commerce Streaming & Medallion Architecture

## üõ†Ô∏è Pr√©-requisitos

Para executar e testar este projeto, voc√™ precisar√° de um dos seguintes ambientes configurados:

* **Ambiente de Desenvolvimento:** VS Code com a extens√£o do Databricks instalada e configurada via **Databricks Connect**.
* **Ambiente de Execu√ß√£o:** Uma conta ativa no Databricks (pode ser a **Community/Free Edition** para testes de l√≥gica, embora o suporte a DLT e Bundles seja completo em inst√¢ncias **Premium/Enterprise**).
* **Databricks CLI:** Instalado e autenticado para realizar o deploy dos Assets (DABs).

## 1. Descri√ß√£o do Projeto
Este projeto implementa um pipeline de dados de ponta a ponta focado em e-commerce, utilizando **Delta Live Tables (DLT)** para processamento em tempo real (Streaming). O sistema √© projetado para escalar a ingest√£o de dados brutos at√© a entrega de camadas refinadas para an√°lise, garantindo integridade e governan√ßa.

## 2. Objetivo
O objetivo principal √© demonstrar a implementa√ß√£o da **Arquitetura Medalh√£o** e do **Change Data Capture (CDC)** no Databricks, automatizando a cria√ß√£o de infraestrutura via **Databricks Asset Bundles (DABs)** e garantindo a qualidade dos dados em cada etapa do fluxo.

## 3. Arquitetura Medalh√£o
A organiza√ß√£o dos dados segue o padr√£o Medalh√£o, garantindo que a qualidade aumente conforme o dado flui pelas camadas:

![Arquitetura Medalh√£o](./Images/Medallion_architecture.png)

| Camada | Prop√≥sito | Qualidade de Dados (Expectations) |
| :--- | :--- | :--- |
| **Bronze** | Ingest√£o bruta via Auto Loader | **WARN**: Monitoramento de inconformidades |
| **Silver** | Valida√ß√£o, limpeza e enriquecimento | **DROP**: Remo√ß√£o autom√°tica de dados inv√°lidos |
| **Gold** | KPIs de neg√≥cio e agrega√ß√µes finais | **FAIL**: Valida√ß√£o rigorosa e cr√≠tica |

### 3.1 Unity Catalog
O **Unity Catalog** atua como a camada unificada de governan√ßa para o Lakehouse.

![Unity Catalog](./Images/Unity_catalog.png)

Ele permite o gerenciamento centralizado de linhagem (*lineage*), controle de acesso (ACLs) e auditoria de todos os ativos de dados (tabelas, volumes e fun√ß√µes) em n√≠vel de metastore.

## 4. Tecnologias Utilizadas
* **Databricks Delta Live Tables (DLT):** Orquestra√ß√£o declarativa e governan√ßa do pipeline.
* **Spark Structured Streaming:** Processamento escal√°vel em tempo real.
* **Unity Catalog:** Governan√ßa unificada e seguran√ßa de dados.
* **Python & SQL:** Desenvolvimento das transforma√ß√µes e l√≥gicas de neg√≥cio.
* **Change Data Capture (CDC):** Captura eficiente de altera√ß√µes em tabelas dimensionais (Clientes, Produtos).
* **Auto Loader:** Ingest√£o incremental e eficiente de arquivos na camada Bronze.
* **Databricks Asset Bundles (DABs):** Ferramenta de infraestrutura como c√≥digo (IaC) para deploy e CI/CD.

## üìä Monitoramento e Execu√ß√£o

Ap√≥s iniciar os processos, voc√™ poder√° acompanhar o status de execu√ß√£o diretamente na interface do Databricks. Abaixo est√£o as visualiza√ß√µes esperadas para cada etapa:

### Execu√ß√£o do Job (Orquestra√ß√£o)
O Job coordena a execu√ß√£o dos bundles e o acionamento dos fluxos de dados.
![Execu√ß√£o do Job](./Images/log_job.png)

### Pipeline Medalh√£o (Streaming DLT)
Visualiza√ß√£o do grafo de depend√™ncias (DAG) processando os dados das camadas Bronze e Silver em tempo real.
![Pipeline Medalh√£o](./Images/pipeline_medallion.png)

### Pipeline CDC (Change Data Capture)
Fluxo de sincroniza√ß√£o de mudan√ßas para manter as tabelas dimensionais atualizadas.
![Pipeline CDC](./Images/pipeline_cdc.png)

---

## 5. Estrutura do Projeto
```text
src/
‚îú‚îÄ‚îÄ pipelines/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/          # Scripts de ingest√£o inicial (Raw)
‚îÇ   ‚îú‚îÄ‚îÄ cdc/             # L√≥gica de processamento CDC
‚îÇ   ‚îî‚îÄ‚îÄ silver/          # Transforma√ß√µes e enriquecimento
‚îú‚îÄ‚îÄ setup/               # Scripts de infraestrutura e permiss√µes
‚îÇ   ‚îú‚îÄ‚îÄ run.py           # Orquestrador de setup inicial
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ utils/               # Utilit√°rios e gerador de dados
    ‚îî‚îÄ‚îÄ data_generator.py
```

## 6. Como Rodar o Projeto
Siga os passos abaixo para configurar o ambiente e realizar o deploy:

### Passo 1: Clonar o Reposit√≥rio
```
git clone [[https://github.com/GabrielGalani/databricks-lakeflow-streamlab.git](https://github.com/seu-usuario/seu-repositorio.git)]
cd databricks-lakeflow-streamlab
```

### Passo 2: Configura√ß√£o Inicial (Setup)
Execute o script de setup para preparar o Unity Catalog (schemas, volumes e permiss√µes):
```python
./src/setup/run.py
```

### Passo 3: Deploy via Databricks Asset Bundles
Certifique-se de que o Databricks CLI est√° configurado e execute o deploy para criar automaticamente os recursos (Workflows e DLT):

### Passo 4: Execu√ß√£o dos Pipelines
Com o deploy conclu√≠do, execute os pipelines na seguinte ordem de depend√™ncia:
- Data Generator: Para popular os volumes com dados iniciais.
- Medallion: Ingest√£o e transforma√ß√£o das camadas Bronze e Silver.
- CDC: Atualiza√ß√£o das dimens√µes com base nas mudan√ßas capturadas.









